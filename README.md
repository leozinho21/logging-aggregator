# logging-aggregator

This is a simple implementation of aggregation/ingestion system for logging events. There is a client that sends randomly generated logging events to a server that accepts the events and pushes them on to Kafka. Another server consumes the log events and writes them to a Cassandra database.

<h5> Defining the conceptual model of the system: </h5>

Log events represented by an application simple logger output messages. Consisted by attributes such as
  <ol>
    <li>Time. This is a timestamp attribute and shows the time log generated.</li>
    <li>Severity (FATAL, ERROR, WARN, INFO, DEBUG,TRACE, ALL, OFF). This is an enum attribute that tells us how severe the message is. Any Severity level its more important than the levels on its right side, meaning that, setting a logger to a specific level will also generate logs for the levels on the right side of the specific selection.</li>
    <li>Source name. This is string attribute and if the log was an application log, i.e java application, it would be the class name from which the log was generated</li>
    <li>Message. String attribute representing the message it self</li>
    <li>Schema Version. Integer attribute value</li>
    <li>In application log events we could add more attributes such as thread,appender etc.</li>
  </ol>
  
# Apache Thrift
Apache Thrift was originally developed by the Facebook development team and is currently maintained by Apache.
Thrift uses a special Interface Description Language (IDL) to define data types and service interfaces which are stored as .thrift files and used later as input by the thrift compiler for generating the source code of client and server software that communicate over different programming languages.

Used latest version 0.13.0 from the <a href="https://search.maven.org/classic/#search%7Cgav%7C1%7Cg%3A%22org.apache.thrift%22%20AND%20a%3A%22libthrift%22">Maven repository</a>.

<h4>Base Types </h4>
<ul>
  <li>bool – a boolean value (true or false)</li>
  <li>byte – an 8-bit signed integer</li>
  <li>i16 – a 16-bit signed integer</li>
  <li>i32 – a 32-bit signed integer</li>
  <li>i64 – a 64-bit signed integer</li>
  <li>double – a 64-bit floating point number</li>
  <li>string – a text string encoded using UTF-8 encoding </li>
</ul>

<h4>Special Types </h4>
<ul>
  <li>binary – a sequence of unencoded bytes</li>
  <li>optional – a Java 8's Optional type</li>
</ul>
  
<h4>Structs</h4>
  
Thrift structs are the equivalent of classes in OOP languages but without inheritance. A struct has a set of strongly typed fields, each with a unique name as an identifier. Fields may have various annotations (numeric field IDs, optional default values, etc.).

<h4> Containers </h4>

Thrift containers are strongly typed containers:

<ul>
  <li>list – an ordered list of elements</li>
  <li>set – an unordered set of unique elements</li>
  <li>map<type1,type2> – a map of strictly unique keys to values
Container elements may be of any valid Thrift type.</li>
</ul>

<h4>Exceptions</h4>

Exceptions are functionally equivalent to structs, except that they inherit from the native exceptions.

<h4>Services</h4>

Services are actually communication interfaces defined using Thrift types. They consist of a set of named functions, each with a list of parameters and a return type.

<h5>Source Code Generation</h5>

Code generated using Maven Plugin maven-thrift-plugin version 0.1.11. Both Thrift-Client and Thrift-Server modules have the same logging.thrift file on src/main/thrift for generating java classes.

One thing about Apache Thrift is that it has its own client-server communication framework which makes communication easy. First we define a transport layer with the implementation of TServerTransport interface (or abstract class, to be more precise). Since we are talking about server, we need to provide a port to listen to. Then we need to define a TServer instance and choose one of the available implementations:

  TSimpleServer – for simple server
  TThreadPoolServer – for multi-threaded server
  TNonblockingServer – for non-blocking multi-threaded server

And provide a processor implementation for chosen server which was already generated by Thrift, LogEventService.Processor class.

From a client perspective, the actions are similar.

Define the transport and point it to our server instance, then choose the suitable protocol. The only difference is that here we initialize the client instance which was, once again, already generated by Thrift, LogEventService.Client class.

Since it is based on .thrift file definitions we can directly call methods described there.

# Apache Cassandra

Cassandra is a scalable NoSQL database that provides continuous availability with no single point of failure and gives the ability to handle large amounts of data with exceptional performance.

This database uses a ring design instead of using a master-slave architecture. In the ring design, there is no master node – all participating nodes are identical and communicate with each other as peers.

This makes Cassandra a horizontally scalable system by allowing for the incremental addition of nodes without needing reconfiguration.

<h4> Key Concepts </h4>

Let’s start with a short survey of some of the key concepts of Cassandra:

<ul>
  <li>Cluster – a collection of nodes or Data Centers arranged in a ring architecture. A name must be assigned to every cluster, which will subsequently be used by the participating nodes</li>
  <li>Keyspace – If you are coming from a relational database, then the schema is the respective keyspace in Cassandra. The keyspace is the outermost container for data in Cassandra. The main attributes to set per keyspace are the Replication Factor, the Replica Placement Strategy and the Column Families</li>
  <li>Column Family – Column Families in Cassandra are like tables in Relational Databases. Each Column Family contains a collection of rows which are represented by a Map<RowKey, SortedMap<ColumnKey, ColumnValue>>. The key gives the ability to access related data together</li>
  <li>Column – A column in Cassandra is a data structure which contains a column name, a value and a timestamp. The columns and the number of columns in each row may vary in contrast with a relational database where data are well structured</li>
</ul>

<h4> Connecting to Cassandra</h4>

Used Cassandra java driver 4.1.0 from <a href="https://mvnrepository.com/artifact/com.datastax.oss/java-driver-core/4.1.0">Maven Repository</a>
In order to connect to Cassandra from Java, we need to build a Cluster object.
An address of a node needs to be provided as a contact point. If we don't provide a port number, the default port (9042) will be used. These settings allow the driver to discover the current topology of a cluster.

<h4> Creating the Keyspace </h4> 

At this part we create the keyspaceName and define two more parameters, the replicationFactor and the replicationStrategy. These parameters determine the number of replicas and how the replicas will be distributed across the ring, respectively.
With replication Cassandra ensures reliability and fault tolerance by storing copies of data in multiple nodes.

In this example for the domain model LogEvent i have chosen a table name logging_events_by_severity and clustering columns event_id,severity,creation_date and source_name. The only call to repository added on code is the selectAll case which returns data ordered ascending first by severity then by creation_date and last by source_name.If we wanted to implement another query we could create a new table with columns that are convenient for reading and replicate the data.
This way, many of the tables in your data model contain duplicate data. This is not a downside of this database. On the contrary, this practice optimizes the performance of the reads.


# Apache Kafka 

Kafka is one of the most popular publisher-subscriber models written in Java and Scala. It was originally developed by LinkedIn and later open-sourced. Kafka is known for handling heavy loads, i.e. I/O operations. You can find out more about Kafka <a href="http://kafka.apache.org/" target="_blank" rel="nofollow">here</a>

<h4>Core concepts</h4>
<ul>
    <li>Kafka is run as a cluster on one or more servers that can span multiple datacenters.</li>
    <li>The Kafka cluster stores streams of <i>records</i> in categories called <i>topics</i>.</li>
    <li>Each record consists of a key, a value, and a timestamp.</li>
</ul>

<p>Kafka has four core APIs:</p>

<ul style="float: left; width: 40%;">
      <li>The <a href="/documentation.html#producerapi">Producer API</a> allows an application to publish a stream of records to one or more Kafka topics.
      </li><li>The <a href="/documentation.html#consumerapi">Consumer API</a> allows an application to subscribe to one or more topics and process the stream of records produced to them.
    </li><li>The <a href="/documentation/streams">Streams API</a> allows an application to act as a <i>stream processor</i>, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.
    </li><li>The <a href="/documentation.html#connect">Connector API</a> allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.
  </li>
</ul>

For the needs of this application only the first two apis used,Producer and Consumer API.

<h4>Creating a Producer</h4>

Code snippet below shows the job done:

<code>
public static Producer<Object, LoggingEvent> createProducer() {
		Properties props = new Properties();
		
		//If Kafka is running in a cluster then you can provide comma (,) separated addresses. 
		//For example:localhost:9091,localhost:9092 
		
		props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ProducerConfig.CLIENT_ID_CONFIG, "thrift-client");
		
		// we add key serializer class even if we dont use it because kafka requires default key class
		props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class.getName());
		props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, SimpleSerializer.class.getName());
		
		return new KafkaProducer<>(props);
	}
</code>

SimpleSerializer is a custom class that will be used to serialize the value object of LoggingEvent (the generated class from Thrift) with the help of jackson api DataMapper class.

<code>
  public class SimpleSerializer implements Serializer<LoggingEvent> {
	
	private Logger logger = LoggerFactory.getLogger(getClass());

	@Override
	public byte[] serialize(String topic, LoggingEvent data) {
		
		byte[] retVal = null;
		
		ObjectMapper objectMapper = new ObjectMapper();
		
		try {
			retVal = objectMapper.writeValueAsString(data).getBytes();
		} catch (Exception exception) {
			logger.error("Error in serializing object [{}],[{}]" , data , exception.getMessage());
		}
		
		return retVal;
	}

}
</code>

<h4>Creating a Consumer</h4>

<code>
  public static KafkaConsumer<Object, LogEvent> createConsumer() {
  
		Properties props = new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.GROUP_ID_CONFIG, "consumerGroup1");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, LoggingEventDeserializer.class.getName());
		props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1);
		props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
            props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

            KafkaConsumer<Object, LogEvent> consumer = new KafkaConsumer<>(props);
            consumer.subscribe(Collections.singletonList("logging-events"));

            return consumer;
	}
  
</code>  

As you see on the code snippet our KafkaConsumer listen to logging-events topic.

<ul> 
 <li>BOOTSTRAP_SERVERS_CONFIG: The Kafka broker's address. If Kafka is running in a cluster then you can provide comma (,) seperated addresses. For example:localhost:9091,localhost:9092.</li> 
 <li>GROUP_ID_CONFIG: The consumer group id used to identify to which group this consumer belongs.</li> 
 <li>KEY_DESERIALIZER_CLASS_CONFIG: Our key is null but we just need to provide a class.For specific implementations you can create your custom deserializer by implementing the <strong>Deserializer</strong> interface provided by Kafka.</li> 
 <li>VALUE_DESERIALIZER_CLASS_CONFIG: The class name to deserialize the value object. We have used json serializing LoggingEvent as the value so we will be using <strong>LoggingEventDeserializer</strong> as the deserializer class./li> 
</ul>

Running the consumer

<code>
	
private void runConsumer() {
		
		Consumer<Object, LogEvent> consumer = createConsumer();
		
		int noMessageFound = 0;
		int giveUp = 100;
		
		while (true) {
			ConsumerRecords<Object, LogEvent> consumerRecords = consumer.poll(Duration.ofMillis(1000));
			// 1000 is the time in milliseconds consumer will wait if no record is found at
			// broker.
			if (consumerRecords.count() == 0) {
				noMessageFound++;
				if (noMessageFound > giveUp)
					// If no message found count is reached to threshold exit loop.
					break;
				else
					continue;
			}

			LogEventsRepository repo = connectToRepository();

			consumerRecords.forEach(record -> {
				UUID uuid = repo.insertLogEvent(record.value());
				logger.debug("Saved record with uuid [{}] Key [{}] partition [{}] offset [{}]", uuid, record.key(),
						record.partition(), record.offset());
			});
			// commits the offset of record to broker.
			consumer.commitAsync();
		}
		consumer.close();
	}
</code>

As the code snippet points out our consumer will try max of 100 times with wait time of 1000ms (1sec), to find new records associated with hes offset,and if any found will save them on repository and commit the offset async.


# Installation

Installation guide is reffered only to linux environment users.

<h4> Software versions </h4>

<ul>
    <li>JDK 8, it is important to use jdk and not jre, because maven compiler requires jdk.</li>
    <li>Thrift 0.13.0 latest </li>
    <li>Kafka 2.11-2.1.1</li>
    <li>Cassandra 3.11.5</li>
    <li>Shell: bash 4.4.20</li>
</ul>

<h4> JDK 8 installation</h4>

To install this version, first update the package index:

<code>sudo apt update</code>

Next, check if Java is already installed:

<code>java -version</code>

If Java is not currently installed, you’ll see the following output:

Command 'java' not found, but can be installed with:

<code>apt install default-jre</code>
<code>apt install openjdk-11-jre-headless</code>
<code>apt install openjdk-8-jre-headless</code>
<code>apt install openjdk-9-jre-headless</code>

To install jdk enter

<code>sudo apt install default-jdk</code>

If you have already installed a jdk but you re using a jre you can switch to jdk by typing
<code>sudo update-alternatives --config java</code>

and selecting according version.

<h4> Installing Thrift </h4>

Thrift is an maven dependency and its managed through maven-thrift-plugin so we don't need to download and install the compiler. An important notice about maven-thrift-plugin is thats its under build->pluginManagement->plugins and not build->plugins like maven-compiler-plugin.

<h4> Installing kafka </h4>

If you don't want to create a dedicated kafka user you should skip step 1.

<h4>Step 1 — Creating a User for Kafka</h4>

Since Kafka can handle requests over a network, you should create a dedicated user for it. This minimizes damage to your Ubuntu machine should the Kafka server be compromised. We will create a dedicated kafka user in this step, but you should create a different non-root user to perform other tasks on this server once you have finished setting up Kafka.

Logged in as your non-root sudo user, create a user called kafka with the useradd command:

<code>sudo useradd kafka -m</code>
The -m flag ensures that a home directory will be created for the user. This home directory, /home/kafka, will act as our workspace directory for executing commands in the sections below.

Set the password using passwd:

<code>sudo passwd kafka</code>
Add the kafka user to the sudo group with the adduser command, so that it has the privileges required to install Kafka’s dependencies:

<code>sudo adduser kafka sudo</code>
Your kafka user is now ready. Log into this account using su:

<code> su -l kafka </code>
Now that we’ve created the Kafka-specific user, we can move on to downloading and extracting the Kafka binaries.

<h4>Step 2 — Downloading and Extracting the Kafka Binaries</h4>

Let’s download and extract the Kafka binaries into dedicated folders in our kafka user’s home directory.

To start, create a directory in /home/kafka called Downloads to store your downloads:

<code>mkdir ~/Downloads</code>

Use curl to download the Kafka binaries:

<code>curl "https://www.apache.org/dist/kafka/2.1.1/kafka_2.11-2.1.1.tgz" -o ~/Downloads/kafka.tgz </code>
Create a directory called kafka and change to this directory. This will be the base directory of the Kafka installation:

<code>mkdir ~/kafka && cd ~/kafka</code>

Extract the archive you downloaded using the tar command:

<code>tar -xvzf ~/Downloads/kafka.tgz --strip 1</code>

We specify the --strip 1 flag to ensure that the archive’s contents are extracted in ~/kafka/ itself and not in another directory (such as ~/kafka/kafka_2.11-2.1.1/) inside of it.

Now that we’ve downloaded and extracted the binaries successfully, we can move on configuring to Kafka to allow for topic deletion.

<h4>Step 3 — Configuring the Kafka Server</h4>

Kafka’s default behavior will not allow us to delete a topic, the category, group, or feed name to which messages can be published. To modify this, let’s edit the configuration file.

Kafka’s configuration options are specified in server.properties. Open this file with nano or your favorite editor:

<code>nano ~/kafka/config/server.properties</code>

Let’s add a setting that will allow us to delete Kafka topics. Add the following to the bottom of the file:

<code>~/kafka/config/server.properties</code>
<code>delete.topic.enable = true</code>

Save the file, and exit nano. Now that we’ve configured Kafka, we can move on to creating systemd unit files for running and enabling it on startup.

<h4>Step 4 — Creating Systemd Unit Files and Starting the Kafka Server</h4>

In this section, we will create systemd unit files for the Kafka service. This will help us perform common service actions such as starting, stopping, and restarting Kafka in a manner consistent with other Linux services.

Zookeeper is a service that Kafka uses to manage its cluster state and configurations. It is commonly used in many distributed systems as an integral component. If you would like to know more about it, visit the official Zookeeper docs.

Create the unit file for zookeeper:

<code>sudo nano /etc/systemd/system/zookeeper.service</code>

Enter the following unit definition into the file: /etc/systemd/system/zookeeper.service

[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
The [Unit] section specifies that Zookeeper requires networking and the filesystem to be ready before it can start.

	
The [Service] section specifies that systemd should use the zookeeper-server-start.sh and zookeeper-server-stop.sh shell files for starting and stopping the service. It also specifies that Zookeeper should be restarted automatically if it exits abnormally.

Next, create the systemd service file for kafka:

<code>sudo nano /etc/systemd/system/kafka.service </code>

Enter the following unit definition into the file: /etc/systemd/system/kafka.service

[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target


The [Unit] section specifies that this unit file depends on zookeeper.service. This will ensure that zookeeper gets started automatically when the kafka service starts.

The [Service] section specifies that systemd should use the kafka-server-start.sh and kafka-server-stop.sh shell files for starting and stopping the service. It also specifies that Kafka should be restarted automatically if it exits abnormally.

Now that the units have been defined, start Kafka with the following command:

<code>sudo systemctl start kafka</code>

To ensure that the server has started successfully, check the journal logs for the kafka unit:

<code>sudo journalctl -u kafka</code>
You should see output similar to the following:

Output
Νοε 22 20:40:45 leonidas-P55A-UD3R systemd[1]: Started kafka.service
You now have a Kafka server listening on port 9092.

While we have started the kafka service, if we were to reboot our server, it would not be started automatically. To enable kafka on server boot, run:

<code>sudo systemctl enable kafka</code>

<h4> Cassandra Installation </h4>

<strong>Adding Cassandra repository to the Ubuntu Source list</strong>

In this step, you will add Cassandra’s repository to Ubuntu’s sources list as shown in the command below:

<code>echo "deb http://www.apache.org/dist/cassandra/debian 311x main" | sudo tee -a 
/etc/apt/sources.list.d/cassandra.sources.list</code>

<strong>Add Apache Cassandra’s repository Keys</strong>
Next, add Cassandra’s repository keys using the command below

<code>curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -</code>

<strong>Install Apache Cassandra</strong>
After appending Cassandra’s repository keys, update the system repositories for the changes to be registered.

<code>sudo apt update</code>
Run the command below to install Cassandra:

<code>sudo apt install cassandra</code>

To check if Cassandra is running execute:

<code>systemctl status cassandra</code>

